{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "num_pipe=make_pipeline(SimpleImputer(strategy=\"mean\"),StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SCD Type 1 Implementation for Employees\") \\\n",
    "    .config(\"spark.jars\", \"C:/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Database connection details\n",
    "db_url = \"jdbc:postgresql://localhost:5432/NewDB\"  # Replace with your host, port, and database\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",      # Replace with your PostgreSQL username\n",
    "    \"password\": \"postgres\",  # Replace with your PostgreSQL password\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Load source (employee) and target (mydata) tables\n",
    "source_table = \"employee\"\n",
    "target_table = \"mydata\"\n",
    "\n",
    "source_df = spark.read.jdbc(url=db_url, table=source_table, properties=db_properties)\n",
    "target_df = spark.read.jdbc(url=db_url, table=target_table, properties=db_properties)\n",
    "\n",
    "# Define the primary key and columns to update\n",
    "key_column = \"EMPLOYEE_ID\"  # Replace with the primary key column name\n",
    "update_columns = [\"FIRST_NAME\", \"LAST_NAME\", \"EMAIL\", \"PHONE_NUMBER\", \"HIRE_DATE\", \"JOB_ID\", \"SALARY\", \"COMMISSION_PCT\", \"MANAGER_ID\", \"DEPARTMENT_ID\"]\n",
    "\n",
    "# Perform SCD Type 1 Logic\n",
    "# Step 1: Join source and target tables\n",
    "joined_df = source_df.alias(\"source\").join(\n",
    "    target_df.alias(\"target\"),\n",
    "    col(f\"source.{key_column}\") == col(f\"target.{key_column}\"),\n",
    "    \"outer\"\n",
    ")\n",
    "\n",
    "# Step 2: Identify rows to update\n",
    "to_update = joined_df.filter(\n",
    "    col(f\"target.{key_column}\").isNotNull() & (\n",
    "        (col(\"source.FIRST_NAME\") != col(\"target.FIRST_NAME\")) |\n",
    "        (col(\"source.LAST_NAME\") != col(\"target.LAST_NAME\")) |\n",
    "        (col(\"source.EMAIL\") != col(\"target.EMAIL\")) |\n",
    "        (col(\"source.PHONE_NUMBER\") != col(\"target.PHONE_NUMBER\")) |\n",
    "        (col(\"source.HIRE_DATE\") != col(\"target.HIRE_DATE\")) |\n",
    "        (col(\"source.JOB_ID\") != col(\"target.JOB_ID\")) |\n",
    "        (col(\"source.SALARY\") != col(\"target.SALARY\")) |\n",
    "        (col(\"source.COMMISSION_PCT\") != col(\"target.COMMISSION_PCT\")) |\n",
    "        (col(\"source.MANAGER_ID\") != col(\"target.MANAGER_ID\")) |\n",
    "        (col(\"source.DEPARTMENT_ID\") != col(\"target.DEPARTMENT_ID\"))\n",
    "    )\n",
    ").select(\"source.*\")\n",
    "\n",
    "# Step 3: Identify rows to insert (new records in source)\n",
    "to_insert = joined_df.filter(col(f\"target.{key_column}\").isNull()).select(\"source.*\")\n",
    "\n",
    "# Step 4: Write updates and inserts back to PostgreSQL\n",
    "# Updates\n",
    "if to_update.count() > 0:\n",
    "    to_update.write \\\n",
    "        .jdbc(url=db_url, table=target_table, mode=\"overwrite\", properties=db_properties)\n",
    "\n",
    "# Inserts\n",
    "if to_insert.count() > 0:\n",
    "    to_insert.write \\\n",
    "        .jdbc(url=db_url, table=target_table, mode=\"append\", properties=db_properties)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
